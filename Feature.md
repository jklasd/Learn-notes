# 特征工程

## 特征工程概述

"数据决定了机器学习的上限，而算法只是尽可能逼近这个上限" \
数据指的就是经过特征工程得到的数据。

特征工程指的是把原始数据转变为模型的训练数据的过程，它的目的就是获取更好的训练数据特征，使得机器学习模型逼近这个上限。

**一般认为括特征构建、特征提取、特征选择三个部分**


### 一、特征构建

特征构建是指从原始数据中人工的找出一些具有物理意义的特征。需要花时间去观察原始数据，思考问题的潜在形式和数据结构，对数据敏感性和机器学习实战经验能帮助特征构建。除此之外，属性分割和结合是特征构建时常使用的方法。结构性的表格数据，可以尝试组合二个、三个不同的属性构造新的特征，如果存在时间相关属性，可以划出不同的时间窗口，得到同一属性在不同时间下的特征值，也可以把一个属性分解或切分，例如将数据中的日期字段按照季度和周期后者一天的上午、下午和晚上去构建特征。总之特征构建是个非常麻烦的问题，书里面也很少提到具体的方法，需要对问题有比较深入的理解。

### 二、特征提取 (_降维_)
PCA、ICA、SOM、MDS、ISOMAP、LLE……

**1、PCA主成分分析**
* 协方差原理

    样本X和样本Y的协方差(Covariance)：
    
    ![[协方差公式]](https://images2017.cnblogs.com/blog/984656/201708/984656-20170830164626405-1764657020.png)        
    
    协方差为正时说明X和Y是正相关关系，协方差为负时X和Y是负相关关系，协方差为0时X和Y相互独立。\
    Cov(X,X)就是X的方差(Variance).当样本是n维数据时，它们的协方差实际上是协方差矩阵（对称方阵），方阵的边长是Cn2。\
    比如对于3维数据(x,y,z)，计算它的协方差就是：
    
    ![[Cov(x,y)的协方差]](https://images2017.cnblogs.com/blog/984656/201708/984656-20170830164644124-930328298.png)
    
* SVD分解原理
    
    若AX=λX，则称λ是A的特征值，X是对应的特征向量。\
    实际上可以这样理解：矩阵A作用在它的特征向量X上，仅仅使得X的长度发生了变化，缩放比例就是相应的特征值λ。
    当A是n阶可逆矩阵时，A与P-1Ap相似，相似矩阵具有相同的特征值。
    
    特别地，当A是对称矩阵时，A的奇异值等于A的特征值，存在正交矩阵Q（Q-1=QT），使得：
    ![[正交矩阵]](https://images2017.cnblogs.com/blog/984656/201708/984656-20170830164731343-1220904233.png)
    
    对A进行奇异值分解就能求出所有特征值和Q矩阵。A∗Q=Q∗D,D是由特征值组成的对角矩阵由特征值和特征向量的定义知，
    Q的列向量就是A的特征向量。
    
* PCA原理及实现

    PCA主要通过把数据从高维映射到低维来降低特征维度。如下图所示，但映射的时候要保留尽量多的主要信息。
    
    ![[PCA]](https://images2017.cnblogs.com/blog/984656/201708/984656-20170830165452593-348310486.png)
    
    **_PCA的算法步骤如下：_**
    
    ---
    - 输入数据集x={x(1)，x(2)，x(3)，.....，x(m)}，需要降到K维；
    - 对所有样本进行均值归一化，如图所示![[]](https://images2017.cnblogs.com/blog/984656/201708/984656-20170830171147905-1678387004.png)
    
    - 计算协方差矩阵![[]](https://images2017.cnblogs.com/blog/984656/201708/984656-20170830171317843-522396617.png)
    - 对协方差矩阵进行奇异值分解![[]](http://latex.codecogs.com/gif.latex?%5BU%2CS%2CV%5D%20%3D%20svd%28%5CSigma%29)
    - 选取最大的前K个特征值对应的特征向量u(1)，u(2)，u(3)，.....，u(k)
    - 输出降维的投影特征矩阵Ureduce={u(1)，u(2)，u(3)，.....，u(k)}
    - 输出降维后的数据集z=**Ureduce** T x

**2、LDA线性判别分析**

**3、ICA独立分析**

**4、因子分析**

### 三、特征选择