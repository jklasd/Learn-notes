# 特征工程

## 特征工程概述

"数据决定了机器学习的上限，而算法只是尽可能逼近这个上限" \
数据指的就是经过特征工程得到的数据。

特征工程指的是把原始数据转变为模型的训练数据的过程，它的目的就是获取更好的训练数据特征，使得机器学习模型逼近这个上限。

   **一般认为括特征构建、特征提取、特征选择三个部分**
 
   ---

### 一、特征构建

特征构建是指从原始数据中人工的找出一些具有物理意义的特征。需要花时间去观察原始数据，思考问题的潜在形式和数据结构，对数据敏感性和机器学习实战经验能帮助特征构建。除此之外，属性分割和结合是特征构建时常使用的方法。结构性的表格数据，可以尝试组合二个、三个不同的属性构造新的特征，如果存在时间相关属性，可以划出不同的时间窗口，得到同一属性在不同时间下的特征值，也可以把一个属性分解或切分，例如将数据中的日期字段按照季度和周期后者一天的上午、下午和晚上去构建特征。总之特征构建是个非常麻烦的问题，书里面也很少提到具体的方法，需要对问题有比较深入的理解。

### 二、特征提取 (_降维_)
PCA、ICA、SOM、MDS、ISOMAP、LLE……

**1、PCA主成分分析(无监督)**
* 协方差原理

    样本X和样本Y的协方差(Covariance)：
    
    ![[协方差公式]](https://images2017.cnblogs.com/blog/984656/201708/984656-20170830164626405-1764657020.png)        
    
    协方差为正时说明X和Y是正相关关系，协方差为负时X和Y是负相关关系，协方差为0时X和Y相互独立。\
    Cov(X,X)就是X的方差(Variance).当样本是n维数据时，它们的协方差实际上是协方差矩阵（对称方阵），方阵的边长是Cn2。\
    比如对于3维数据(x,y,z)，计算它的协方差就是：
    
    ![[Cov(x,y)的协方差]](https://images2017.cnblogs.com/blog/984656/201708/984656-20170830164644124-930328298.png)
    
* SVD分解原理
    
    若AX=λX，则称λ是A的特征值，X是对应的特征向量。\
    实际上可以这样理解：矩阵A作用在它的特征向量X上，仅仅使得X的长度发生了变化，缩放比例就是相应的特征值λ。
    当A是n阶可逆矩阵时，A与P-1Ap相似，相似矩阵具有相同的特征值。
    
    特别地，当A是对称矩阵时，A的奇异值等于A的特征值，存在正交矩阵Q（Q-1=QT），使得：
    ![[正交矩阵]](https://images2017.cnblogs.com/blog/984656/201708/984656-20170830164731343-1220904233.png)
    
    对A进行奇异值分解就能求出所有特征值和Q矩阵。A∗Q=Q∗D,D是由特征值组成的对角矩阵由特征值和特征向量的定义知，
    Q的列向量就是A的特征向量。
    
* PCA原理及实现

    PCA主要通过把数据从高维映射到低维来降低特征维度。如下图所示，但映射的时候要保留尽量多的主要信息。
    
    ![[PCA]](https://images2017.cnblogs.com/blog/984656/201708/984656-20170830165452593-348310486.png)
    
    **_PCA的算法步骤如下：_**
    
   ---
    - 输入数据集x={x(1)，x(2)，x(3)，.....，x(m)}，需要降到K维；
    - 对所有样本进行均值归一化，如图所示![[]](https://images2017.cnblogs.com/blog/984656/201708/984656-20170830171147905-1678387004.png)
    
    - 计算协方差矩阵![[]](https://images2017.cnblogs.com/blog/984656/201708/984656-20170830171317843-522396617.png)
    - 对协方差矩阵进行奇异值分解![[]](http://latex.codecogs.com/gif.latex?%5BU%2CS%2CV%5D%20%3D%20svd%28%5CSigma%29)
    - 选取最大的前K个特征值对应的特征向量u(1)，u(2)，u(3)，.....，u(k)
    - 输出降维的投影特征矩阵 Ureduce={u(1)，u(2)，u(3)，.....，u(k)}
    - 输出降维后的数据集 z=**Ureduce** T x

**2、LDA线性判别分析(有监督)**

   - 概念
     
     Linear Discriminant Analysis (也有叫做Fisher Linear Discriminant)。与PCA一样，是一种线性降维算法。不同于PCA只会选择数据变化最大的方向，由于LDA是有监督的（分类标签），所以LDA会主要以类别为思考因素，使得投影后的样本尽可能可分。它通过在k维空间选择一个投影超平面，使得不同类别在该超平面上的投影之间的距离尽可能近，同时不同类别的投影之间的距离尽可能远。从而试图明确地模拟数据类之间的差异。
    
   - 算法
   
     在LDA中，我们假设每一个类别的数据服从高斯分布，且具有相同协方差矩阵Σ。 
     为了得到最优的分类器，我们需要知道类别的后验概率P(Ck|x)。根据贝叶斯定理：\
     ![[]](https://img-blog.csdn.net/20171213150522931) \
     其中，πk  是类别Ck的先验概率，是已知的，那么主要就是求出类条件概率密度函数fk(x)。
     不同的算法，对这个类条件概率密度函数的假设都不同。
     
   - sklearn提供的API
     
     sklearn的discriminant_analysis提供了LDA方法LinearDiscriminantAnalysis
     
         def __init__(self, solver='svd', shrinkage=None, priors=None,
                          n_components=None, store_covariance=False, tol=1e-4):
                 self.solver = solver
                 self.shrinkage = shrinkage
                 self.priors = priors
                 self.n_components = n_components
                 self.store_covariance = store_covariance  # used only in svd solver
                 self.tol = tol  # used only in svd solver

     solver ：即求LDA超平面特征矩阵使用的方法。可以选择的方法有奇异值分解"svd"，最小二乘"lsqr"和特征分解"eigen"。一般来说特征数非常多的时候推荐使用svd，而特征数不多的时候推荐使用eigen。主要注意的是，如果使用svd，则不能指定正则化参数shrinkage进行正则化。默认值是svd
     
     shrinkage ：正则化参数，可以增强LDA分类的泛化能力。如果仅仅只是为了降维，则一般可以忽略这个参数。默认是None，即不进行正则化。可以选择"auto",让算法自己决定是否正则化。当然我们也可以选择不同的[0,1]之间的值进行交叉验证调参。注意shrinkage只在solver为最小二乘"lsqr"和特征分解"eigen"时有效。
     
     priors  ：类别权重，可以在做分类模型时指定不同类别的权重，进而影响分类模型建立。降维时一般不需要关注这个参数。
     
     n_components：即我们进行LDA降维时降到的维数。在降维时需要输入这个参数。注意只能为[1,类别数-1)范围之间的整数。如果我们不是用于降维，则这个值可以用默认的None。
     
**3、ICA独立分析**

**4、因子分析**

### 三、特征选择